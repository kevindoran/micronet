{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import efficientnet.efficientnet_builder as efnet_builder\n",
    "import efficientnet.efficientnet_model as efnet_model\n",
    "# import util\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.WARN)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# Set locale for printing (e.g. where to place integer separator ','s).\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, '')  # Use '' for auto, or force e.g. to 'en_US.UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential layers, class filter and contribution ranked feature pruning\n",
    "The hypothesis is that the following three techniques can significantly reduce \n",
    "computation requirements of image classification neural networks:\n",
    "  1. Compute a subset of the global average pool input (subset of 7x7 layer \n",
    "     output for EfficientNet). Sequentially compute outputs until classification \n",
    "     confidence is achieved.\n",
    "  2. Determine the stochastic contributions of layer outputs to subsequent layer\n",
    "     activations. Ignore inputs that don't meet a certain threshold. \n",
    "  3. Use a small sized network operating on a scaled down input image to \n",
    "     calculate with high accuracy the top 10 classes. This is hoped to reduce\n",
    "     the need to compute features in the previous layers: compute only 50% of \n",
    "     2nd last layer outputs (feature layer), 75% of 3rd last layer, 87.5% of 4th\n",
    "     last layer. The actual reduction relationship is not known.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequential pooling\n",
    "If there is a reliable pattern of activation of layer 'block 15', the layer with\n",
    "7x7xfiters outputs connecting to the global pool, this pattern could be used to\n",
    "determine a subset of the 49 output 'pixels' \\[What are the 'pixels' of the \n",
    "inner layers called?\\] that can be computed with minimal effect on \n",
    "classification accuracy. \n",
    "\n",
    "More precisely, we could set a minimum required accuracy (in expectation) and\n",
    "compute only a subset of the 49 pixels if one of the following sets of\n",
    "functions can be well approximated. Each set needs to address "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping condition\n",
    "We need a mapping from current activations to expected accuracy degradation. \n",
    "Pixels should continue to be calculated until the minimum expected accuracy is\n",
    "reached. The available state data we have is the 49x1280 neuron outputs and the \n",
    "1280 pooled outputs. Possible mappings could be:\n",
    "\n",
    "  1. g(num_pixels_evaluated | order) = accuracy_estimate\n",
    "  2. total_pool_output = f(state data). g(total_pool_output) = accuracy_estimate\n",
    "  3. f(pixels_evaluated) = contribution_estimate. g(contribution) = accuracy_estimate\n",
    "  \n",
    "An intermediate quantity that might be a good proxy for estimating accuracy is\n",
    "the cross-entropy between the normal classification ouput and the perturbed \n",
    "classification output.\n",
    "  \n",
    "For the above 3 mappings, we would need to measure and parameterize the \n",
    "following distributions:\n",
    "\n",
    "  1. (X, Y): (num pixels evaluated, accuracy | eval order)\n",
    "  2. (X, Y): (total pool output, accuracy)\n",
    "  3. (X, Z): (pixel, contribution), (Z, Y): (expected contribution sum, accuracy)\n",
    "  \n",
    "These distributions require the following information: \n",
    "\n",
    "  1. (correct classification?, num pixels evaluated)\n",
    "  2. (correct classifciation?, total pool output)\n",
    "  3. (correct classification?, percentage contribution)\n",
    "          [49] (pixel, contribution)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel evaluation order\n",
    "Which pixels should be calculated next so as to maximize accuracy. Naive orders\n",
    "could be sequential from top-left to bottom right, or a random order.\n",
    "\n",
    "\n",
    "### Dependency on class activations\n",
    "With 1280 neurons connected densely to the pool layer, it is not feasible to \n",
    "calculate the 1280 class activations each time a pool value is updated. However,\n",
    "if #3 above is implemented, the smaller number of considered classes would allow\n",
    "the class activations to be computed multiple times without a significant effect \n",
    "on overall operation count. \n",
    "\n",
    "\n",
    "## Experiment 1: test sequential pooling with accuracy estimate based on the number of pixels evaluated\n",
    "Success criteria: \n",
    "    * discover that dropping random pixels has minimal effect on the accuracy \n",
    "      for small drop counts.\n",
    "    * extra: discover that some specific pixels have minimal effect on accuracy \n",
    "      (i.e. classification depends disproportionately on certain pixels).\n",
    "      \n",
    "      \n",
    "Data collection\n",
    "Run large number of evaluations (100,000). For every evaluation, run 49 extra\n",
    "evaluations with 1 dropped pixel. \n",
    "results = [] \n",
    "for i in range 100,000:\n",
    "    logits = run_eval()\n",
    "    l = logit_for_correct(logits, ans)\n",
    "    for j in range 49:\n",
    "        for k in (0,1)\n",
    "        mask = ave_mask if k else zero_mask\n",
    "        mask_pixel(j, mask)\n",
    "        logits = run_eval()\n",
    "        results.append(j, mask, l, logit_for_correct(logits, ans), correct)\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results (experiment id: 2.1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original accuracy: 0.75146484\n",
      "max masked accuracy: 0.7651367\n",
      "min masked accuracy: 0.76049805\n"
     ]
    }
   ],
   "source": [
    "res = {'accuracy': 0.75146484, 'mask_accuracy_18': 0.7624512, 'mask_accuracy_40': 0.76342773, 'mask_accuracy_26': 0.7631836, 'mask_accuracy_35': 0.7651367, 'mask_accuracy_29': 0.7624512, 'mask_accuracy_43': 0.76416016, 'mask_accuracy_27': 0.7626953, 'mask_accuracy_10': 0.76220703, 'mask_accuracy_16': 0.7619629, 'mask_accuracy_11': 0.76123047, 'mask_accuracy_46': 0.7636719, 'mask_accuracy_48': 0.76342773,  'mask_accuracy_2': 0.7631836, 'mask_accuracy_31': 0.7619629, 'mask_accuracy_37': 0.7644043, 'mask_accuracy_36': 0.76464844, 'mask_accuracy_23': 0.76171875, 'mask_accuracy_47': 0.76293945, 'mask_accuracy_45': 0.7644043, 'mask_accuracy_34': 0.7626953, 'mask_accuracy_38': 0.76464844, 'mask_accuracy_0': 0.7626953, 'mask_accuracy_30': 0.76220703, 'mask_accuracy_20': 0.7626953, 'mask_accuracy_1': 0.7609863, 'mask_accuracy_22': 0.7624512, 'mask_accuracy_39': 0.7644043, 'mask_accuracy_24': 0.7624512, 'mask_accuracy_8': 0.7626953, 'mask_accuracy_6': 0.7607422, 'mask_accuracy_7': 0.76342773, 'mask_accuracy_13': 0.7624512, 'mask_accuracy_19': 0.76171875, 'mask_accuracy_25': 0.76293945, 'mask_accuracy_9': 0.7619629, 'mask_accuracy_14': 0.7631836, 'mask_accuracy_28': 0.76293945, 'mask_accuracy_44': 0.76464844, 'mask_accuracy_17': 0.7619629, 'mask_accuracy_33': 0.7631836, 'mask_accuracy_15': 0.76293945, 'mask_accuracy_5': 0.76049805, 'mask_accuracy_42': 0.76464844, 'mask_accuracy_32': 0.7636719, 'mask_accuracy_3': 0.7624512, 'mask_accuracy_4': 0.76123047, 'mask_accuracy_21': 0.76171875, 'mask_accuracy_41': 0.763916, 'mask_accuracy_12': 0.76171875}\n",
    "masked_accuracies = tuple(res['mask_accuracy_' + str(a)] for a in range(49))\n",
    "plt.bar(range(49), masked_accuracies)\n",
    "plt.show()\n",
    "print(f'original accuracy: {res[\"accuracy\"]}')\n",
    "print(f'max masked accuracy: {max(masked_accuracies)}')\n",
    "print(f'min masked accuracy: {min(masked_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Zero-ing a single pixel in the 7x7 grid of features consistently produces higher accuracy.\n",
    "\n",
    "This is a strange result, one that raises the question: was the\n",
    "experiment flawed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "Maybe PCA is a way to get some measures that don't ignore the dependencies \n",
    "between different neuron outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
